# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

"""

import numpy as np

#training inputs

x=np.array([[0,0,1],
            [0,1,1],
            [1,0,1],
            [1,1,1]])

y=np.array([[0],
            [1],
            [1],
            [0]])

#sigmoid

def sigmoid(z):
  return(1/(1+np.exp(-z)))

#sigmoid derivative

def sigmoid_prime(z):
  return (z*(1-z))

np.random.seed(1)

alpha = 0.4
batch = 10000

a0 = x

w0 = 2*np.random.random((3,4)) -1
w1 = 2*np.random.random((4,1)) -1

#cross entropy loss function
import math
def crossent(z):
  return -math.log(1 - z)

for cnt in range(batch):
  batch_x = x
  batch_y = y
  n=batch_x.shape[0]
  #first layer activation value
  a0 = batch_x
  
  #feedforward operation for the first layer
  z1 = np.dot(a0,w0)
  a1 = sigmoid(z1)

  z2 = np.dot(a1,w1)
  a2 = sigmoid(z2)
  
  #cross ent loss function instead of mse
  c2 = np.vectorize(crossent) 
  l2_error = c2(a2)

  if cnt % 1000 == 0:
    print('Error: ' + str(np.mean(np.mean(np.abs(l2_error)))))

  l2_delta = l2_error * sigmoid_prime(a2)

  l1_error = l2_delta.dot(w1.T)

  l1_delta = l2_delta.dot(w1.T)
  #weight update

  w1 -= alpha*a1.T.dot(l2_delta)
  w0 -= alpha*a0.T.dot(l1_delta)

#test function
def test_NN(z):
  k1 = np.dot(z,w0)
  j1 = sigmoid(k1)

  k2 = np.dot(j1,w1)
  j2 = sigmoid(k2)

  return j2

#given datas
x1 = np.array([1,1,0])
x2 = np.array([1,1,1])

#output y1 for x1 data
test_NN(x1)

#output y2 for x2 data
test_NN(x2)